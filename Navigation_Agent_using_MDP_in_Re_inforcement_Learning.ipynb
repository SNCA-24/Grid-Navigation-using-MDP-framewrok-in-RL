{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SNCA-24/Grid-Navigation-using-MDP-framewrok-in-RL/blob/main/Navigation_Agent_using_MDP_in_Re_inforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Starter Code"
      ],
      "metadata": {
        "id": "dpPYJ2YlwKVy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OE83YdA_wJGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#graph creation\n",
        "def create_graph(rows, cols, obstacles):\n",
        "    graph = {}\n",
        "\n",
        "    for r in range(1, rows + 1):\n",
        "        for c in range(1, cols + 1):\n",
        "            if (r, c) not in obstacles:\n",
        "                neighbors = []\n",
        "                for dr, dc in [(0, 1), (0, -1), (1, 0), (-1, 0)]:  # Right, Left, Up, Down\n",
        "                    nr, nc = r + dr, c + dc\n",
        "                    if 1 <= nr <= rows and 1 <= nc <= cols and (nr, nc) not in obstacles:\n",
        "                        neighbors.append((nr, nc))\n",
        "                graph[(r, c)] = neighbors\n",
        "\n",
        "    return graph\n",
        "\n",
        "# Define grid parameters\n",
        "rows, cols = 5, 5\n",
        "obstacles = [(3, 2), (3, 4), (5, 5)]\n",
        "destination = (3, 5)\n",
        "hazards = [(1, c) for c in range(1, 6)]  # Bottom row\n",
        "\n",
        "# Create the graph\n",
        "grid_graph = create_graph(rows, cols, obstacles)\n",
        "\n",
        "# Print the graph to verify\n",
        "for position, neighbors in grid_graph.items():\n",
        "    print(f\"{position}: {neighbors}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5rvXyjRjj3t",
        "outputId": "235ed462-3488-4482-89bb-4c8d70ca96b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 1): [(1, 2), (2, 1)]\n",
            "(1, 2): [(1, 3), (1, 1), (2, 2)]\n",
            "(1, 3): [(1, 4), (1, 2), (2, 3)]\n",
            "(1, 4): [(1, 5), (1, 3), (2, 4)]\n",
            "(1, 5): [(1, 4), (2, 5)]\n",
            "(2, 1): [(2, 2), (3, 1), (1, 1)]\n",
            "(2, 2): [(2, 3), (2, 1), (1, 2)]\n",
            "(2, 3): [(2, 4), (2, 2), (3, 3), (1, 3)]\n",
            "(2, 4): [(2, 5), (2, 3), (1, 4)]\n",
            "(2, 5): [(2, 4), (3, 5), (1, 5)]\n",
            "(3, 1): [(4, 1), (2, 1)]\n",
            "(3, 3): [(4, 3), (2, 3)]\n",
            "(3, 5): [(4, 5), (2, 5)]\n",
            "(4, 1): [(4, 2), (5, 1), (3, 1)]\n",
            "(4, 2): [(4, 3), (4, 1), (5, 2)]\n",
            "(4, 3): [(4, 4), (4, 2), (5, 3), (3, 3)]\n",
            "(4, 4): [(4, 5), (4, 3), (5, 4)]\n",
            "(4, 5): [(4, 4), (3, 5)]\n",
            "(5, 1): [(5, 2), (4, 1)]\n",
            "(5, 2): [(5, 3), (5, 1), (4, 2)]\n",
            "(5, 3): [(5, 4), (5, 2), (4, 3)]\n",
            "(5, 4): [(5, 3), (4, 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##R1"
      ],
      "metadata": {
        "id": "UcCxscvbMllh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttRB7tIrMeBk",
        "outputId": "e8215ba5-988b-4897-f3ab-1f06b50d2999"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a) Best live-in reward (r): -0.10000000000000003\n",
            "\n",
            "b) Best policy P1 (arrows):\n",
            "→ → → ↓  \n",
            "→ → → → ↓\n",
            "↑   ↑   D\n",
            "↑ → ↑ → ↑\n",
            "H H H H H\n",
            "\n",
            "Numeric values after Value Iteration:\n",
            "  8.83   9.03   9.24   9.46   0.00\n",
            "  8.98   9.22   9.46   9.70   9.95\n",
            "  8.64   0.00   9.14   0.00   0.00\n",
            "  7.45   7.08   8.20   7.72   8.88\n",
            "  0.00   0.00   0.00   0.00   0.00\n"
          ]
        }
      ],
      "source": [
        "# R1 Code\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Environment setup\n",
        "rows, cols = 5, 5\n",
        "obstacles = [(3, 2), (3, 4), (5, 5)]\n",
        "destination = (3, 5)\n",
        "hazards = [(1, c) for c in range(1, 6)]\n",
        "\n",
        "# MDP parameters\n",
        "gamma = 0.99\n",
        "epsilon = 0.001\n",
        "max_iterations = 100\n",
        "noise = 0.1  # Updated noise factor\n",
        "\n",
        "def create_graph():\n",
        "#Create a graph representation of the grid with valid transitions.\n",
        "    graph = defaultdict(list)\n",
        "    for r in range(1, rows + 1):\n",
        "        for c in range(1, cols + 1):\n",
        "            if (r, c) not in obstacles:\n",
        "                for dr, dc in [(0, 1), (0, -1), (1, 0), (-1, 0)]:  # Right, Left, Up, Down\n",
        "                    nr, nc = r + dr, c + dc\n",
        "                    if 1 <= nr <= rows and 1 <= nc <= cols and (nr, nc) not in obstacles:\n",
        "                        graph[(r, c)].append((nr, nc))\n",
        "    return graph\n",
        "\n",
        "def get_reward(state):\n",
        "# Get the reward for a given state.\n",
        "    if state == destination:\n",
        "        return 10\n",
        "    elif state in hazards:\n",
        "        return -10\n",
        "    else:\n",
        "        return 0  # Default reward\n",
        "\n",
        "def get_transition_probabilities(state, action, graph):\n",
        "# Get transition probabilities considering noise.\n",
        "\n",
        "    probs = defaultdict(float)\n",
        "    intended_state = action\n",
        "    valid_actions = graph[state]\n",
        "\n",
        "    if intended_state in valid_actions:\n",
        "        probs[intended_state] = 1 - noise\n",
        "        other_actions = [a for a in valid_actions if a != intended_state]\n",
        "        for a in other_actions:\n",
        "            probs[a] = noise / len(other_actions)\n",
        "    else:\n",
        "        for a in valid_actions:\n",
        "            probs[a] = 1.0 / len(valid_actions)\n",
        "\n",
        "    return probs\n",
        "\n",
        "def value_iteration(graph, r):\n",
        "# Perform value iteration for a given live-in reward r.\n",
        "    V = {state: 0 for state in graph}\n",
        "    for _ in range(max_iterations):\n",
        "        delta = 0\n",
        "        for state in graph:\n",
        "            if state == destination or state in hazards:\n",
        "                continue\n",
        "            v = V[state]\n",
        "            values = []\n",
        "            for action in graph[state]:\n",
        "                value = 0\n",
        "                for next_state, prob in get_transition_probabilities(state, action, graph).items():\n",
        "                    reward = r if get_reward(next_state) == 0 else get_reward(next_state)\n",
        "                    value += prob * (reward + gamma * V[next_state])\n",
        "                values.append(value)\n",
        "            V[state] = max(values)\n",
        "            delta = max(delta, abs(v - V[state]))\n",
        "        if delta < epsilon:\n",
        "            break\n",
        "    return V\n",
        "\n",
        "def extract_policy(V, graph, r, destination, hazards, obstacles):\n",
        "    policy = {}\n",
        "    for state in graph:\n",
        "        if state == destination:\n",
        "            policy[state] = 'D'\n",
        "        elif state in hazards:\n",
        "            policy[state] = 'H'\n",
        "        elif state in obstacles:\n",
        "            policy[state] = 'X'\n",
        "        else:\n",
        "            values = []\n",
        "            for action in graph[state]:\n",
        "                value = 0\n",
        "                for next_state, prob in get_transition_probabilities(state, action, graph).items():\n",
        "                    reward = r if get_reward(next_state) == 0 else get_reward(next_state)\n",
        "                    value += prob * (reward + gamma * V[next_state])\n",
        "                values.append(value)\n",
        "            best_action = graph[state][np.argmax(values)]\n",
        "            dr, dc = best_action[0] - state[0], best_action[1] - state[1]\n",
        "            if (dr, dc) == (0, 1):\n",
        "                policy[state] = '→'\n",
        "            elif (dr, dc) == (0, -1):\n",
        "                policy[state] = '←'\n",
        "            elif (dr, dc) == (1, 0):\n",
        "                policy[state] = '↑'\n",
        "            elif (dr, dc) == (-1, 0):\n",
        "                policy[state] = '↓'\n",
        "    return policy\n",
        "\n",
        "graph = create_graph()\n",
        "\n",
        "best_r = None #The optimal live-in reward value\n",
        "best_policy = None #The optimal policy\n",
        "best_value_sum = float('-inf') # The highest sum of state values\n",
        "best_V = None # optimal value function\n",
        "\n",
        "for r in np.arange(-0.3, -0.05, 0.05):\n",
        "    V = value_iteration(graph, r)\n",
        "    policy = extract_policy(V, graph, r, destination, hazards, obstacles)\n",
        "    value_sum = sum(V.values())\n",
        "    if value_sum > best_value_sum:\n",
        "        best_value_sum = value_sum\n",
        "        best_r = r\n",
        "        best_policy = policy\n",
        "        best_V = V\n",
        "\n",
        "print(f\"a) Best live-in reward (r): {best_r}\")\n",
        "print(\"\\nb) Best policy P1 (arrows):\")\n",
        "for i in range(rows, 0, -1):\n",
        "    row = [best_policy.get((i, j), ' ') for j in range(1, cols + 1)]\n",
        "    print(' '.join(row))\n",
        "print(\"\\nNumeric values after Value Iteration:\")\n",
        "for i in range(rows, 0, -1):\n",
        "    row = [f\"{best_V.get((i, j), 0):6.2f}\" for j in range(1, cols + 1)]\n",
        "    print(' '.join(row))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if we wanted to include the reward which is at position 3,3 the we can update the get reward code in this way\n",
        "def get_reward(state):\n",
        "    if state == destination:\n",
        "        return 10\n",
        "    elif state == (3, 3):\n",
        "        return 1\n",
        "    elif state in hazards:\n",
        "        return -10\n",
        "    else:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "TI9RO6cH1UG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##R2"
      ],
      "metadata": {
        "id": "Lnt0fhGvMj_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "# Environment setup\n",
        "rows, cols = 5, 5\n",
        "destination = (3, 5)\n",
        "hazards = [(1, c) for c in range(1, 6)]\n",
        "\n",
        "# MDP parameters\n",
        "gamma = 0.99\n",
        "epsilon = 0.001\n",
        "max_iterations = 100\n",
        "\n",
        "def create_graph(obstacles):\n",
        "    graph = defaultdict(list)\n",
        "    for r in range(1, rows + 1):\n",
        "        for c in range(1, cols + 1):\n",
        "            if (r, c) not in obstacles:\n",
        "                for dr, dc in [(0, 1), (0, -1), (1, 0), (-1, 0)]:  # Right, Left, Up, Down\n",
        "                    nr, nc = r + dr, c + dc\n",
        "                    if 1 <= nr <= rows and 1 <= nc <= cols and (nr, nc) not in obstacles:\n",
        "                        graph[(r, c)].append((nr, nc))\n",
        "    return graph\n",
        "\n",
        "def get_reward(state):\n",
        "    if state == destination:\n",
        "        return 10\n",
        "    elif state in hazards:\n",
        "        return -10\n",
        "    else:\n",
        "        return 0  # Will be replaced by live-in reward r\n",
        "\n",
        "def get_transition_probabilities(state, action, graph):\n",
        "# Get transition probabilities considering noise.\n",
        "\n",
        "    probs = defaultdict(float)\n",
        "    intended_state = action\n",
        "    valid_actions = graph[state]\n",
        "\n",
        "    if intended_state in valid_actions:\n",
        "        probs[intended_state] = 1 - noise\n",
        "        other_actions = [a for a in valid_actions if a != intended_state]\n",
        "        for a in other_actions:\n",
        "            probs[a] = noise / len(other_actions)\n",
        "    else:\n",
        "        for a in valid_actions:\n",
        "            probs[a] = 1.0 / len(valid_actions)\n",
        "\n",
        "    return probs\n",
        "def value_iteration(graph, r):\n",
        "# Perform value iteration for a given live-in reward r.\n",
        "\n",
        "\n",
        "    V = {state: 0 for state in graph}\n",
        "    for _ in range(max_iterations):\n",
        "        delta = 0\n",
        "        for state in graph:\n",
        "            if state == destination or state in hazards:\n",
        "                continue\n",
        "            v = V[state]\n",
        "            values = []\n",
        "            for action in graph[state]:\n",
        "                value = 0\n",
        "                for next_state, prob in get_transition_probabilities(state, action, graph).items():\n",
        "                    reward = r if get_reward(next_state) == 0 else get_reward(next_state)\n",
        "                    value += prob * (reward + gamma * V[next_state])\n",
        "            values.append(value)\n",
        "            V[state] = max(values)\n",
        "            delta = max(delta, abs(v - V[state]))\n",
        "        if delta < epsilon:\n",
        "            break\n",
        "    return V\n",
        "\n",
        "\n",
        "#  Extract the policy from the value function.\n",
        "\n",
        "def extract_policy(V, graph, r, destination, hazards, obstacles):\n",
        "    policy = {}\n",
        "    for state in graph:\n",
        "        if state == destination:\n",
        "            policy[state] = 'D'\n",
        "        elif state in hazards:\n",
        "            policy[state] = 'H'\n",
        "        elif state in obstacles:\n",
        "            policy[state] = 'X'\n",
        "        else:\n",
        "            values = []\n",
        "            for action in graph[state]:\n",
        "                value = 0\n",
        "                for next_state, prob in get_transition_probabilities(state, action, graph).items():\n",
        "                    reward = r if get_reward(next_state) == 0 else get_reward(next_state)\n",
        "                    value += prob * (reward + gamma * V[next_state])\n",
        "                values.append(value)\n",
        "            best_action = graph[state][np.argmax(values)]\n",
        "            dr, dc = best_action[0] - state[0], best_action[1] - state[1]\n",
        "            if (dr, dc) == (0, 1):\n",
        "                policy[state] = '→'\n",
        "            elif (dr, dc) == (0, -1):\n",
        "                policy[state] = '←'\n",
        "            elif (dr, dc) == (1, 0):\n",
        "                policy[state] = '↑'\n",
        "            elif (dr, dc) == (-1, 0):\n",
        "                policy[state] = '↓'\n",
        "    return policy\n",
        "\n",
        "# Create P1\n",
        "obstacles_P1 = [(3, 2), (3, 4), (5, 5)]\n",
        "graph_P1 = create_graph(obstacles_P1)\n",
        "\n",
        "# Find the best live-in reward\n",
        "best_r = None\n",
        "best_value_sum = float('-inf')\n",
        "\n",
        "for r in np.arange(-0.3, -0.05, 0.05):\n",
        "    V = value_iteration(graph_P1, r)\n",
        "    value_sum = sum(V.values())\n",
        "    if value_sum > best_value_sum:\n",
        "        best_value_sum = value_sum\n",
        "        best_r_P1 = r\n",
        "\n",
        "V_P1 = value_iteration(graph_P1, best_r_P1)\n",
        "P1 = extract_policy(V_P1, graph_P1, best_r_P1,  destination, hazards, obstacles_P1)\n",
        "\n",
        "# Create P2\n",
        "obstacles_P2 = [(3, 2), (3, 4), (4, 4)]  # Changed from (5, 5) to (4, 4)\n",
        "graph_P2 = create_graph(obstacles_P2)\n",
        "\n",
        "for r in np.arange(-0.3, -0.05, 0.05):\n",
        "    V = value_iteration(graph_P2, r)\n",
        "    value_sum = sum(V.values())\n",
        "    if value_sum > best_value_sum:\n",
        "        best_value_sum = value_sum\n",
        "        best_r_P2 = r\n",
        "\n",
        "V_P2 = value_iteration(graph_P2, best_r_P2)\n",
        "P2 = extract_policy(V_P2, graph_P2, best_r_P2,  destination, hazards, obstacles_P2)\n",
        "\n",
        "def visualize_policies(P1, P2, rows, cols):\n",
        "# Visualize the original and modified policies side by side.\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "    for ax, policy, title, obstacles in zip(axes, [P1, P2], [\"Policy P1\", \"Policy P2\"],[obstacles_P1,obstacles_P2]):\n",
        "        # Create a grid to display the policies\n",
        "        grid = np.full((rows, cols), ' ')\n",
        "        for r in range(1, rows + 1):\n",
        "            for c in range(1, cols + 1):\n",
        "                grid[r - 1, c - 1] = policy.get((r, c), ' ')\n",
        "\n",
        "        # Setting up the visualization grid\n",
        "        ax.imshow(np.zeros((rows, cols)), cmap=\"Greys\", alpha=0.3)\n",
        "        for r in range(rows):\n",
        "            for c in range(cols):\n",
        "                # Place the text in the center of each cell\n",
        "                ax.text(c, rows - r - 1, grid[r, c],\n",
        "                        fontsize=14, ha='center', va='center')\n",
        "\n",
        "        # Adding gridlines and adjust layout\n",
        "        ax.set_xticks(np.arange(-0.5, cols, 1), minor=True)\n",
        "        ax.set_yticks(np.arange(-0.5, rows, 1), minor=True)\n",
        "        ax.grid(which='minor', color='black', linestyle='-', linewidth=0.5)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        ax.set_title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "print(\"\\nPolicy P1:\")\n",
        "for i in range(rows, 0, -1):\n",
        "    row = [P1.get((i, j), ' X') for j in range(1, cols + 1)]\n",
        "    print(' '.join(row))\n",
        "\n",
        "print(\"\\nPolicy P2:\")\n",
        "for i in range(rows, 0, -1):\n",
        "    row = [P2.get((i, j), 'X ') for j in range(1, cols + 1)]\n",
        "    print(' '.join(row))\n",
        "\n",
        "print(\"\\nDifferences between P1 and P2:\")\n",
        "for i in range(1, rows + 1):\n",
        "    for j in range(1, cols + 1):\n",
        "        if P1.get((i, j), ' ') != P2.get((i, j), ' '):\n",
        "            print(f\"Position ({i},{j}): P1 = {P1.get((i, j), ' ')}, P2 = {P2.get((i, j), ' ')}\")\n",
        "print('\\n')\n",
        "visualize_policies(P1, P2, rows, cols)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Bh4rBixrDQ1M",
        "outputId": "185819e0-2a41-4110-8c55-13b07e3392aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Policy P1:\n",
            "→ → → ↓  X\n",
            "→ → → → ↓\n",
            "↑  X ↑  X D\n",
            "↑ → → → ↑\n",
            "H H H H H\n",
            "\n",
            "Policy P2:\n",
            "→ → → → ↓\n",
            "→ ↑ ↑ X  ↓\n",
            "↑ X  ↑ X  D\n",
            "↑ → → → ↑\n",
            "H H H H H\n",
            "\n",
            "Differences between P1 and P2:\n",
            "Position (4,2): P1 = →, P2 = ↑\n",
            "Position (4,3): P1 = →, P2 = ↑\n",
            "Position (4,4): P1 = →, P2 =  \n",
            "Position (5,4): P1 = ↓, P2 = →\n",
            "Position (5,5): P1 =  , P2 = ↓\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABJ4AAAJkCAYAAABQyuuvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAnhElEQVR4nO3df5DU9XnA8ef4EYuJhxkhcQhEo6mQiW1qqpTOJHBn48SQ5kZN8MxosEmTmrY2aJ2mJRQiwRqTZkjt9MZMaiOkwU6cthrppQxqxI6ZpjDFRqIRctdCwLQEZ/TAihyH2z+sWAqBPY6H732++3rN8IfL3d7DPPfd/fhm92hrNBqNAAAAAIATbEzVAwAAAABQT8ITAAAAACmEJwAAAABSCE8AAAAApBCeAAAAAEghPAEAAACQQngCAAAAIIXwBAAAAEAK4QkAAACAFMITkKajoyM6OjoO/vfWrVujra0tVqxYUdlMAACjnTMUUCfCE3DQihUroq2t7eCvn/u5n4vzzjsvrr/++ti5c2fV4x2XdevWHfJnGj9+fJxzzjkxf/78+Pd///dDPvaOO+6IefPmxZvf/OZoa2uL3/iN36hmaACgKK18htq+fXssXbo0Zs6cGa9//etj0qRJ0dHREQ8++GCF0wOjybiqBwBGn8997nPxlre8JV588cV49NFH44477ohvf/vb8YMf/CBOPfXU477fs846K/bu3Rvjx48/gdM251Of+lRcdNFFsX///ti4cWN89atfjd7e3ti0aVNMmTIlIiK+8IUvxJ49e2LmzJnxn//5nyd9RgCgbK14hvrWt74VX/jCF+Kyyy6La6+9NoaGhuLrX/96XHLJJfG1r30tPvrRj570mYHRRXgCDvO+970vLrzwwoiI+PjHPx5nnHFGLF++PL71rW/Fhz/84eO+31f+BrAK7373u+NDH/pQRER89KMfjfPOOy8+9alPxcqVK2PhwoUREfHII48cfLXT6173ukrmBADK1YpnqM7Ozvjxj38ckyZNOvg5n/zkJ+OXfumXYsmSJcIT4K12wLFdfPHFERHxH//xHxERMTQ0FMuWLYtzzz03TjnllDj77LPjM5/5TOzbt++o9/Ozfj7BU089FVdeeWVMnjw5JkyYENOnT49FixZFRMTDDz8cbW1tce+99x52f3fffXe0tbXFP//zP4/4zxTx8t8mtrW1Dfu+AACOpBXOUG9/+9sPiU4REaecckrMnTs3duzYEXv27Bn21wDqRXgCjqm/vz8iIs4444yIePlv8JYsWRLvfOc748tf/nLMmTMnPv/5z8dVV1017Pt+/PHH41d+5VfiO9/5TnziE5+I22+/PS677LJYvXp1RLz8wzWnTZsWq1atOuxzV61aFeeee2786q/+6oj/TAAAJ1orn6H+67/+K0499dQRvcUQqAdvtQMOMzAwEM8880y8+OKL8d3vfjc+97nPxYQJE+LXf/3X4/vf/36sXLkyPv7xj8df/uVfRkTE7/zO78Qb3vCG+NKXvhQPP/xwdHZ2Nv21fu/3fi8ajUZs3Lgx3vzmNx+8/bbbbouIl19afs0118Ty5ctjYGAgJk6cGBERu3btirVr1x78W71j2bNnTzzzzDOxf//+eOyxx2LBggXR1tYWH/zgB5ueFQDgaJyhXtbX1xd///d/H/PmzYuxY8c2/WcC6skrnoDDvOc974nJkyfHtGnT4qqrrorXve51ce+998ab3vSm+Pa3vx0REb//+79/yOfcdNNNERHR29vb9NfZtWtX/NM//VN87GMfO+TAFBGHvOVt/vz5sW/fvvjbv/3bg7d985vfjKGhobjmmmua+lof+9jHYvLkyTFlypR4//vfH//93/8dK1euPPhzGAAARsoZKuKFF16IefPmxYQJEw5GMKC1ecUTcJienp4477zzYty4cfHGN74xpk+fHmPGvNypt23bFmPGjIm3vvWth3zOmWeeGaeffnps27at6a/zyj/Fe/755x/142bMmBEXXXRRrFq1Kn7zN38zIl5+ifisWbMOm+NnWbJkSbz73e+OsWPHxqRJk+Jtb3tbjBvnIRAAOHFa/Qx14MCBuOqqq+LJJ5+Mf/zHfzz4LwcDrc3/dQGHmTlz5jFfCXSyfwj3/PnzY8GCBbFjx47Yt29ffO9734u/+Iu/aPrzf+EXfiHe8573JE4IALS6Vj9DfeITn4h/+Id/iFWrVh38IeQA3moHDMtZZ50VL730UvzoRz865PadO3fGc889F2eddVbT93XOOedERMQPfvCDY37sVVddFWPHjo2/+Zu/iVWrVsX48eOju7t7eMMDAFSk7meoP/iDP4i77rorvvzlL8eHP/zhE37/QLmEJ2BY5s6dGxERf/Znf3bI7cuXL4+IiPe///1N39fkyZNj9uzZ8bWvfS1+/OMfH/J7jUbjkP+eNGlSvO9974tvfOMbsWrVqrj00ksP+6d7AQBGqzqfof70T/80vvSlL8VnPvOZWLBgwQm9b6B83moHDMs73vGOuPbaa+OrX/1qPPfcczFnzpxYv359rFy5Mi677LJh/WssERF//ud/Hu9617vine98Z/zWb/1WvOUtb4mtW7dGb29v/Nu//dshHzt//vz40Ic+FBERy5YtO1F/pINWr14d3//+9yMiYv/+/fH444/HLbfcEhERXV1d8Yu/+Isn/GsCAK2hrmeoe++9Nz796U/Hz//8z8fb3va2+MY3vnHI719yySXxxje+8YR+TaAswhMwbHfeeWecc845sWLFirj33nvjzDPPjIULF8ZnP/vZYd/XO97xjvje974XixcvjjvuuCNefPHFOOuss+LKK6887GM/8IEPxOtf//p46aWXoqur60T8UQ7xd3/3d7Fy5cqD//3YY4/FY489FhERU6dOFZ4AgBGp4xnqlb+0+9GPfhQf+chHDvv9hx9+WHiCFtfW+P+vxQQYpYaGhmLKlCnxgQ98IP7qr/6q6nEAAIrgDAVUyc94Aopx3333xa5du2L+/PlVjwIAUAxnKKBKXvEEjHr/8i//Eo8//ngsW7YsJk2aFBs3bqx6JACAUc8ZChgNvOIJGPXuuOOO+O3f/u14wxveEF//+terHgcAoAjOUMBo4BVPAAAAAKTwiicAAAAAUghPAAAAAKQYN9xPeOmll+InP/lJnHbaadHW1pYxEwDASdNoNGLPnj0xZcqUGDMm5+/knJ8AgLpp9gzVdHjq6emJnp6eGBwcjP7+/hMyJADAaLF9+/aYOnXqCb1P5ycAoO6OdYYa9g8XHxgYiNNPPz22b98e7e3tIx6QanR3d8c3v/nNqsdghOyxfHZYD/ZYtt27d8e0adPiueeei4kTJ6Z8Deen+nC9l88O68Eey2eH5Wv2DDXst9q98vLw9vZ2B6eCjR8/3v5qwB7LZ4f1YI/1kPkWOOen+nC9l88O68Eey2eH9XGsM5QfLg4AAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBkGbBggVx5plnVj0GAACcVM7BrxKeAEgzMDAQO3furHoMAAA4qZyDXyU8AQAAAJBCeDqKwcHBuP7662Pbtm1Vj8II2GP57BCgLB63y2eH9WCP5bND6kB4OoqNGzfGnXfeGbNnz47+/v6qx+E42WP57BCgLB63y2eH9WCP5bND6kB4OopZs2bF6tWrY9euXTF79uzYvHlz1SNxHOyxfHYIUBaP2+Wzw3qwx/LZIXUwruoBqrJ48eJ49tlnm/rY888/PzZs2BBz5syJRx55JKZPn548Hc2yx/LZIUBZPG6Xzw7rwR7LZ4e0ipYNT3fddVc8/fTTw/qcnTt3xpNPPukiH0XssXx2CFAWj9vls8N6sMfy2SGtomXfardjx45oNBrH/PX8889HR0dHREQsXbo0Lr/88moH5xD2WD47BCiLx+3y2WE92GP57JBW0bLhqRl79uyJSy+9NNatWxe33XZbLFmypOqROA72WD47LMvGjRvjK1/5ymG3//CHP4zbb7+9gomAk83jdvnssB7ssXx2WBbn4CNr2bfaNaOvry82bdoUy5cvjxtvvLHqcThO9lg+OyzLokWLYs2aNfHCCy8cvO2JJ56Iiy++OPbu3RtXXHFFTJs2rcIJgWwet8tnh/Vgj+Wzw7I4Bx+Z8HQUF1xwQfT19cWkSZOqHoURsMfy2WFZ7rnnnpg7d27cdNNNMXny5IiI6OzsjMHBwVi7dm1LPtlCq/G4XT47rAd7LJ8dlsU5+Mi81e4YXOD1YI/ls8NynHbaabFmzZro7OyMXbt2RUTE0NBQPPjggzFr1qyKpwNOFo/b5bPDerDH8tlhOZyDj0x4AuCEe+1rXxu9vb1xySWXxBlnnBEPPfRQXHjhhVWPBQAAqZyDD+etdgCkmDBhQqxdu7bqMQAA4KRyDj6UVzwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkGJcsx/Y09MTPT09ceDAgYiI6O7ujvHjx6cNRq7169dHV1dX1WMwQvZYPjusB3ss2/79+9Pu2/mpflzv5bPDerDH8tlh+Zo9Q7U1Go3GcO549+7dMXHixBgYGIj29vbjGo7qdXV1xf3331/1GIyQPZbPDuvBHst2Ms42zk/14Xovnx3Wgz2Wzw7L1+z5xlvtAAAAAEghPAEAAACQQngCAAAAIIXwBAAAAEAK4QkAAACAFMITAAAAACmEJwAAAABSCE8AAAAApBCeAAAAAEghPAEAAACQQngCAAAAIIXwBAAAAEAK4QkAAACAFMITAAAAACmEJwAAAABSCE8AAAAApBCeAAAAAEghPAEAAACQQngCAAAAIIXwBAAAAEAK4QkAAACAFMITAAAAACmEJwAAAABSCE8AAAAApBCeAAAAAEghPAEAAACQQngCAAAAIIXwBAAAAEAK4QkAAACAFMITAAAAACmEJwAAAABSCE8AAAAApBCeAAAAAEghPAEAAACQQngCAAAAIIXwBAAAAEAK4QkAAACAFMITAAAAACmEJwAAAABSCE8AAAAApBCeAAAAAEghPAEAAACQQngCAAAAIIXwBAAAAEAK4QkAAACAFMITAAAAACmEJwAAAABSCE8AwM+0YMGCOPPMM6seA0g2NDQUV199dcyYMSO2bNlS9TgcJ3uE0cMZ6lXCEwDwMw0MDMTOnTurHgNINDg4GPPmzYu77747Nm/eHB0dHfHUU09VPRbDZI8wujhDvUp4AgCAFrVv37644oor4r777ov29vaIiHj++eejo6MjnnjiiYqno1n2CIxmwtNRDA4OxvXXXx/btm2rehRGwB7LZ4f1YI/QOlzv5eju7o7e3t5YuHBhXH755RERsXbt2ti7d290dnbGjh07Kp6QZtgjMJoJT0excePGuPPOO2P27NnR399f9TgcJ3ssnx3Wgz1C63C9l+OGG26IW265JW699daDt82aNSseeOCBuO6662Lq1KkVTkez7BEYzYSno5g1a1asXr06du3aFbNnz47NmzdXPRLHwR7LZ4f1YI/QOlzv5ejo6IhFixYddvvMmTNj2bJlFUzE8bBHYDQbV/UAVVm8eHE8++yzTX3s+eefHxs2bIg5c+bEI488EtOnT0+ejmbZY/nssB7sEVqH6x0AGI6WDU933XVXPP3008P6nJ07d8aTTz7p0DSK2GP57LAe7BFah+sdABiOln2r3Y4dO6LRaBzz1yv/GkRExNKlSw/+sD5GB3ssnx3Wgz1C63C9AwDD0bLhqRl79uyJSy+9NNatWxe33XZbLFmypOqROA72WD47rAd7LMfGjRvjK1/5ymG3//CHP4zbb7+9gokojesdgFbkDHVkLftWu2b09fXFpk2bYvny5XHjjTdWPQ7HyR7LZ4f1YI/lWLRoUaxZsyZeeOGFg7c98cQTcfHFF8fevXvjiiuuiGnTplU4IaOd6x2AVuQMdWTC01FccMEF0dfXF5MmTap6FEbAHstnh/Vgj+W45557Yu7cuXHTTTfF5MmTIyKis7MzBgcHY+3atS15YGJ4XO8AtCJnqCPzVrtjcGCqB3ssnx3Wgz2W4bTTTos1a9ZEZ2dn7Nq1KyIihoaG4sEHH4xZs2ZVPB2lcL0D0GqcoY5MeAIADvPa1742ent745JLLokzzjgjHnroobjwwgurHgsAYFRzhjqc8AQAHNGECRNi7dq18cwzz8QFF1xQ9ThAshUrVkSj0ah6DEbIHqF6zlCHEp4AAAAASCE8AQAAAJBCeAIAAAAghfAEAAAAQArhCQAAAIAUwhMAAAAAKYQnAAAAAFIITwAAAACkEJ4AAAAASCE8AQAAAJBCeAIAAAAghfAEAAAAQArhCQAAAIAUwhMAAAAAKYQnAAAAAFIITwAAAACkEJ4AAAAASCE8AQAAAJBCeAIAAAAghfAEAAAAQArhCQAAAIAUwhMAAAAAKYQnAAAAAFIITwAAAACkEJ4AAAAASCE8AQAAAJBCeAIAAAAghfAEAAAAQArhCQAAAIAUwhMAAAAAKYQnAAAAAFIITwAAAACkEJ4AAAAASCE8AQAAAJBCeAIAAAAghfAEAAAAQArhCQAAAIAUwhMAAAAAKYQnAAAAAFIITwAAAACkEJ4AAAAASCE8AQAAAJBCeAIAAAAghfAEAAAAQArhCQAAAIAUwhMAAAAAKYQnAAAAAFIITwAAAACkEJ4AAAAASCE8AQAAAJBiXLMf2NPTEz09PXHgwIGIiOju7o7x48enDUau9evXR1dXV9VjMEL2WD47rAd7LNv+/fvT7tv5qX5c7+Wzw3qwx/LZYfmaPUO1NRqNxnDuePfu3TFx4sQYGBiI9vb24xqO6nV1dcX9999f9RiMkD2Wzw7rwR7LdjLONs5P9eF6L58d1oM9ls8Oy9fs+cZb7QAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphKf/Y2hoKK6++uqYMWNGbNmypepxoGW5FqE6W7dujba2tkN+nXrqqTFlypT4tV/7tViyZEn09/dXPSajjMdtGB1ci1AN56ejG1f1AKPF4OBgdHd3x3333RcRER0dHfGd73wnZsyYUe1g0GJcizA6nHvuuXHNNddERMS+ffvipz/9aaxfvz6WLVsWt956a3z605+OP/mTP4m2traKJ6VqHrdhdHAtQvWcn45MeIqXvyE++MEPRm9vb7S3t8fu3bvj+eefj46OjnjooYfi7W9/e9UjQktwLcLo8da3vjVuvvnmw25/9NFH4yMf+Uh8/vOfj7Fjx8ayZctO/nCMGh63YXRwLcLo4Px0ZN5qFxHd3d3R29sbCxcujMsvvzwiItauXRt79+6Nzs7O2LFjR8UTQmtwLcLo9653vSvWrFkTp5xySnzxi1+M7du3Vz0SFfK4DaODaxFGt1Y/PwlPEXHDDTfELbfcErfeeuvB22bNmhUPPPBAXHfddTF16tQKp4PW4VqEMkyfPj2uvPLKGBwcPPiWDlqTx20YHVyLMPq18vnJW+3i5fc/d3R0HHb7zJkzY+bMmSd/IGhRrkUoR0dHR/z1X/91bNiwoepRqJDHbRgdXItQhlY9P3nFEwAwbFOmTImIiGeeeabiSQAAytCq5yfhCQAAAIAUwhMAMGw/+clPIiJi8uTJFU8CAFCGVj0/CU8AwLCtW7cuIiIuuuiiagcBAChEq56fhCcAYFi2bNkS99xzT5xyyikH/9luAAB+tlY+PwlPAEDTvvvd78Z73/ve2LdvX/zRH/1RvOlNb6p6JACAUa3Vz0/jqh4AABh9+vr64uabb46IiMHBwfjpT38a69evj02bNsXYsWPjj//4j+Ozn/1stUMCAIwizk9HJjwBAIfp7++PpUuXRkTEhAkT4vTTT48ZM2bE4sWL49prr41zzz234gkBAEYX56cjE57+nxUrVsSKFSuqHgNanmsRqnH22WdHo9GoegwK5HEbRgfXIpx8zk9H52c8AQAAAJBCeAIAAAAghfAEAAAAQArhCQAAAIAUwhMAAAAAKYQnAAAAAFIITwAAAACkEJ4AAAAASCE8AQAAAJBCeAIAAAAghfAEAAAAQArhCQAAAIAUwhMAAAAAKYQnAAAAAFIITwAAAACkEJ4AAAAASCE8AQAAAJBCeAIAAAAghfAEAAAAQArhCQAAAIAUwhMAAAAAKYQnAAAAAFIITwAAAACkEJ4AAAAASCE8AQAAAJBCeAIAAAAghfAEAAAAQArhCQAAAIAUwhMAAAAAKYQnAAAAAFIITwAAAACkEJ4AAAAASCE8AQAAAJBCeAIAAAAghfAEAAAAQArhCQAAAIAUwhMAAAAAKYQnAAAAAFIITwAAAACkEJ4AAAAASCE8AQAAAJBCeAIAAAAghfAEAAAAQArhCQAAAIAUwhMAAAAAKYQnAAAAAFIITwAAAACkEJ4AAAAASCE8AQAAAJBCeAIAAAAgxbhmP7Cnpyd6enriwIEDERHR3d0d48ePTxuMXOvXr4+urq6qx2CE7LF8dlgP9li2/fv3p92381P9uN7LZ4f1YI/ls8PyNXuGams0Go3h3PHu3btj4sSJMTAwEO3t7cc1HNXr6uqK+++/v+oxGCF7LJ8d1oM9lu1knG2cn+rD9V4+O6wHeyyfHZav2fONt9oBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQnv6PoaGhuPrqq2PGjBmxZcuWqscBgEp5XqRZvlcA4FWeFw8lPP2vwcHBmDdvXtx9992xefPm6OjoiKeeeqrqsQCgEp4XaZbvFQB4lefFwwlPEbFv37644oor4r777ov29vaIiHj++eejo6MjnnjiiYqnA4CTy/MizfK9AgCv8rx4ZMJTRHR3d0dvb28sXLgwLr/88oiIWLt2bezduzc6Oztjx44dFU/ISAwODsb1118f27Ztq3oUjpMd1oM9lsPzIs3yvVJfHrPrwR7LZ4dl8bx4ZMJTRNxwww1xyy23xK233nrwtlmzZsUDDzwQ1113XUydOrXC6RipjRs3xp133hmzZ8+O/v7+qsfhONhhPdhjOTwv0izfK/XlMbse7LF8dlgWz4tHJjxFREdHRyxatOiw22fOnBnLli2rYCJOpFmzZsXq1atj165dMXv27Ni8eXPVIzFMdlgP9lgOz4s0y/dKfXnMrgd7LJ8dlsXz4pGNq3oAGInFixfHs88+29THnn/++bFhw4aYM2dOPPLIIzF9+vTk6WiGHdaDPQKUw2N2Pdhj+eyQViE8UbS77rornn766WF9zs6dO+PJJ5/0YD1K2GE92CNAOTxm14M9ls8OaRXeakfRduzYEY1G45i/XvmXBCIili5devAHvVE9O6wHewQoh8fserDH8tkhrcIrnqi9PXv2xNy5c+PRRx+N2267Lf7wD/+w6pEYJjusB3sEKIfH7Hqwx/LZIXUgPFF7fX19sWnTpli+fHnceOONVY/DcbDDerBHgHJ4zK4HeyyfHVIHwhO1d8EFF0RfX19MmjSp6lE4TnZYD/YIUA6P2fVgj+WzQ+rAz3iiJXigLp8d1oM9ApTDY3Y92GP57JDSCU8AAAAApBCe/p8VK1ZEo9GoegwAGBU8L9Is3ysA8CrPi68SngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkGJcsx/Y09MTPT09ceDAgYiI6O7ujvHjx6cNRq7169dHV1dX1WMwQvZYPjusB3ss2/79+9Pu2/mpflzv5bPDerDH8tlh+Zo9Q7U1Go3GcO549+7dMXHixBgYGIj29vbjGo7qdXV1xf3331/1GIyQPZbPDuvBHst2Ms42zk/14Xovnx3Wgz2Wzw7L1+z5xlvtAAAAAEghPAEAAACQQngCAAAAIIXwBAAAAEAK4QkAAACAFMITAAAAACmEJwAAAABSCE8AAAAApBCeAAAAAEghPAEAAACQQngCAAAAIIXwBAAAAEAK4QkAAACAFMITAAAAACmEJwAAAABSCE8AAAAApBCeAAAAAEghPAEAAACQQngCAAAAIIXwBAAAAEAK4QkAAACAFMITAAAAACmEJwAAAABSCE8AAAAApBCeAAAAAEghPAEAAACQQngCAAAAIIXwBAAAAEAK4QkAAACAFMITAAAAACmEJwAAAABSCE8AAAAApBCeAAAAAEghPAEAAACQQngCAAAAIIXwBAAAAEAK4QkAAACAFMITAAAAACmEJwAAAABSCE8AAAAApBCeAAAAAEghPAEAAACQQngCAAAAIIXwBAAAAEAK4QkAAACAFMITAAAAAClaPjxt3bo12tra4tJLL/2ZH7Nu3bpoa2uLT37ykydxMobDHstnh/Vgj+WzQ5rle6V8dlgP9lg+OyyfHR5dy4cnAAAAAHIITwAAAACkEJ4AAAAASCE8AQAAAJBiXNUDjBZ9fX1x8803H/H3tm7delJn4fjZY/nssB7ssXx2SLN8r5TPDuvBHstnh+WzwyMTnv5Xf39/LF26tOoxGCF7LJ8d1oM9ls8OaZbvlfLZYT3YY/nssHx2eGTeave/3vve90aj0Tjir4cffrjq8WiSPZbPDuvBHstnhzTL90r57LAe7LF8dlg+Ozwy4QkAAACAFMITAAAAACmEJwAAAABSCE8AAAAApBCeAAAAAEghPAEAAACQYlzVA1Tt7LPPjkajcdSP6ejoOObHUC17LJ8d1oM9ls8OaZbvlfLZYT3YY/nssHx2eHRe8QQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABACuEJAAAAgBTCEwAAAAAphCcAAAAAUghPAAAAAKQQngAAAABIITwBAAAAkEJ4AgAAACCF8AQAAABAinHD/YRGoxEREbt37z7hw3Dy7N+/3w5rwB7LZ4f1YI9le2V3r5xxMjg/1YfrvXx2WA/2WD47LF+zZ6i2RpOnrJ6enujp6YnBwcHo7+8f+YQAAKPI9u3bY+rUqSf0Pp2fAIC6O9YZqunw9IqXXnopzjvvvPjXf/3XaGtrG/GAVOOiiy6KDRs2VD0GI2SP5bPDerDHsjUajfjlX/7l2LJlS4wZk/NTCJyf6sP1Xj47rAd7LJ8dlq/ZM9Sw32o3ZsyYeM1rXhMTJ04c0YBUa+zYsdHe3l71GIyQPZbPDuvBHsv3mte8Ji06RTg/1YnrvXx2WA/2WD47rIdmzlDHdcL63d/93eMaiNHDDuvBHstnh/Vgj+U7GTv0fVIP9lg+O6wHeyyfHdZDM3sc9lvtAAAAAKAZea8pBwAAAKClCU8AAAAApBCeAAAAAEghPAEAAACQQngCAAAAIIXwBAAAAEAK4QkAAACAFMITAAAAACn+B+dqSpRpdGdJAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##R3"
      ],
      "metadata": {
        "id": "6LWjBqimMrB8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "moa0Y6fLMzzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pseudo-Code for Modification for dynamic environments"
      ],
      "metadata": {
        "id": "X3vxqzJwA9DO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#R3\n",
        "\n",
        "#To adapt MDP for dynamic environment we need to update the following functions in the way those can handle multiple mobile agents\n",
        "#for multi agent environment\n",
        "#state: position of robot, action: moving direction, occupancy_probs: probability occupied by the robot\n",
        "\n",
        "def get_transition_probs(state, action, graph, occupancy_probs):\n",
        "    probs = defaultdict(float) #store transition probabilities\n",
        "    intended_state = action\n",
        "    valid_actions = graph[state]\n",
        "#robot choosen state\n",
        "    if intended_state in valid_actions:\n",
        "        prob_free = 1 - occupancy_probs[intended_state] #probability for the intended state is free\n",
        "        probs[intended_state] = 0.9 * prob_free\n",
        "        probs[state] += 0.9 * (1 - prob_free)\n",
        "#all possible actions from their position (both valid and invalid) and adjust probabilities\n",
        "        other_actions = [a for a in valid_actions if a != intended_state]\n",
        "        for a in other_actions:\n",
        "            prob_free_other = 1 - occupancy_probs[a]\n",
        "            probs[a] = 0.1 * prob_free_other / len(other_actions)\n",
        "            probs[state] += 0.1 * (1 - prob_free_other) / len(other_actions)\n",
        "    else:# if intended actions are invalid the probability is given to valid actions\n",
        "        for a in valid_actions:\n",
        "            prob_free = 1 - occupancy_probs[a]\n",
        "            probs[a] = prob_free / len(valid_actions)\n",
        "            probs[state] += (1 - prob_free) / len(valid_actions)\n",
        "\n",
        "    return probs\n",
        "#value iteration function\n",
        "def dynamic_value_iteration(graph, r, occupancy_probs):\n",
        "    V = {state: 0 for state in graph} #initializing the value function\n",
        "    for _ in range(max_iterations):\n",
        "        delta = 0\n",
        "        for state in graph:\n",
        "            if state == destination or state in hazards:\n",
        "                continue\n",
        "            v = V[state]\n",
        "            values = []\n",
        "            for action in graph[state]:\n",
        "                value = 0\n",
        "                for next_state in graph:\n",
        "                    # P(s'|s,a) this include probability of other robots positions\n",
        "                    prob =get_transition_probs(state, action, next_state)\n",
        "                    # R(s,a,s') may include collision penalties\n",
        "                    r = r(state, action, next_state)\n",
        "                    value += prob * (r + gamma * V[next_state])\n",
        "                values.append(value)\n",
        "            V[state] = max(values)\n",
        "            delta = max(delta, abs(v - V[state]))\n",
        "        if delta < epsilon:\n",
        "            break\n",
        "    return V\n"
      ],
      "metadata": {
        "id": "2Ul1tnVyAUFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for R3\n",
        "we can deal with multi mobile agents environment:\n",
        "\n",
        "*   by extending the state space to have positions for all robots.\n",
        "*   by updating the transition function in this way we can handle obstacles\n",
        "*  by Including the probability of cell occupancy of other robots into decision-making.\n",
        "*   Periodically changing the policy by considering the new environment info.\n",
        "*   Balancing between efficient path planning and collision avoidance.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LCEYvzmNTpEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kfvcWX1NrEW7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YJkMcsC7rEhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ver4EZjOrFPg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}